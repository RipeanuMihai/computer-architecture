\begin{frame}
    \frametitle{Viratul Memory}
    Benefits of Virtual Memory:
    \begin{itemize}
        \item Memory management
        \item Protection
        \item Share memory
        \item Process reallocation
        \item Process creation and start-up time (without loading the entire process in memory)
    \end{itemize}
    The process of getting the physical address from the virtual address is called translation.
    The operating system replaces the virtual memory.
\end{frame}

\begin{frame}
    \frametitle{Allocation Policies}
    \begin{itemize}
        \item Paged virtual memory: The virtual memory is divided into pages.
        \item Segmented virtual memory: The virtual memory is divided into segments.
        \item Combined virtual memory: The virtual memory is divided into segments of size multiple of page size.
    \end{itemize}
    The miss penalty is the time to get the page from the disk, so the operating system must minimize the number of misses.
    There is a full associativity type of memory, where the page can be placed anywhere in the main memory.
\end{frame}


\begin{frame}
    \frametitle{TLB vs MMU}

    \begin{itemize}
        \item \textbf{Translation Lookaside Buffer (TLB)}
        \begin{itemize}
            \item Specialized cache for speeding up virtual to physical address translation.
            \item Stores recent translations.
            \item Checked first during address translation.
            \item Very fast, small size.
        \end{itemize}
        \item \textbf{Memory Management Unit (MMU)}
        \begin{itemize}
            \item Hardware component for managing memory and caching operations.
            \item Translates virtual addresses to physical addresses using page tables.
            \item Handles page faults and memory protection.
            \item More complex and integrated into the CPU.
        \end{itemize}
    \end{itemize}
    
    \begin{table}[h!]
        \centering
        \begin{tabular}{|p{2cm}|p{3cm}|p{3cm}|}
            \hline
            \textbf{Component} & \textbf{Function} & \textbf{Characteristics} \\
            \hline
    TLB & Caches recent address translations & Fast, small size \\
            \hline
    MMU & Manages memory, performs address translations & Complex, integrated into CPU \\
            \hline
        \end{tabular}
    \end{table}

\end{frame}

\begin{frame}
    \frametitle{Exam Questions}
    \begin{itemize}
        \item If I increase the block size to reduce the miss rate, what happens to the CPU execution time?
        \item If I increase the cache size to reduce the miss rate, what happens to the CPU execution time?
        \item If I increase the associativity to reduce the miss rate, what happens to the CPU execution time?
        \item If I add multiple levels of cache, what happens to the CPU execution time?
    \end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Block size increase}
    We have a cache of size $n$ and a block size of $b_{0}$.
    The miss rate is $m_{0}$ and the miss penalty is $p_{access}$ plus $p_{byte}$
    for each byte in clock cycles. If we increase the block size to $b_{1}$,
    the miss rate will be $m_{1}$. Compute Average memory access time (AMAT)
    for both cases. The hit time is $h$.
\end{frame}
\begin{frame}
    \frametitle{Block size increase}
\end{frame}


\begin{frame}
    \frametitle{Cache size increase}
    We have a block size of $b_{0}$ and a miss rate of $m_{0}$.
    The miss penalty is $p_{access}$ plus $p_{byte}$ for each byte in clock cycles.
    If we increase the cache size to $n_{1}$, the miss rate will be $m_{1}$.
    Compute Average memory access time (AMAT) for both cases.
    The hit time is $h_{0}$ for the first cache and $h_{1}$ for the second one.
\end{frame}
\begin{frame}
    \frametitle{Cache size increase}
\end{frame}

\begin{frame}
    \frametitle{Associativity increase}
    We have a cache of size $n$ and a block size of $b_{0}$ with a $k_{0}$-way associativity.
    The miss rate is $m_{0}$, and the miss penalty is $p_{access}$ plus $p_{byte}$ for
    each byte in clock cycles. If we increase the associativity to $k_{1}$,
    the miss rate will be $m_{1}$, and will increase the clock cycle time from $c_{0}$ to $c_{1}$.
    Hit time is $h$ for both. Compute Average memory access time (AMAT) for both cases.
\end{frame}
\begin{frame}
    \frametitle{Associativity increase}
\end{frame}

\begin{frame}
    \frametitle{Multiple levels of cache}
    Suppose that in $m$ memory references, there are $m_{1}$ misses in the first cache and $m_{2}$ misses in the second cache.
    The hit time for the first cache is $h_{1}$, and for the second cache is $h_{2}$.
    The miss penalty for the second cache is $p_{2}$.
    Compute the Average memory access time (AMAT).
\end{frame}
\begin{frame}
    \frametitle{Multiple levels of cache}
\end{frame}